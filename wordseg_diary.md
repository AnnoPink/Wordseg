####2018.12.29 Copy code from jieba  
明天思考一下如何读取文档，并按照标点符号切割文本，生成`test_sent`，进行全篇分词

####2018.12.31 Create 'cut_word()'
完成内容：
>（1）读文件`135.txt`，清洗数据   
>（2）创建新文件`135_分词后.txt`，分词并写入 

待完成项：
>（1）整理出适合教育政策的`userdict.txt`  
>（2）对新创建文件`135_分词后.txt`做词频统计，看统计结果√2019.1.1   
>（3）查一些词频可视化的代码或者软件√2019.1.3 

####2019.1.1  Create  'count_word()'
完成内容：
>（1）对新创建文件`135_分词后.txt`做词频统计   

待完成项：
>（1）修改`count_word()`中频数排序部分及写入代码（抄来的，太繁琐√2019.1.2    
>（2）修改`cut_word()``count_word()`，想办法用string传递读写文件名。分词结果和文件的存储要分开，才三个函数已经要重复分词两遍。jieba分词结果貌似只能join一次seg_list，并不懂为什么？√2019.1.5   

####2019.1.2  Fix 'count_word()'
完成内容：
>（1）简化'count_word()'，3行替换7行，开心！    
>（2）搞清楚"序列（列表，元组）和字典"的区别  

####2019.1.3  Create  'cloud_word_test()'
完成内容：
>（1）懵懵地把词云做出来了，基本流程知道。还需要更进一步了解，出来的图很丑。。。。  

待完成项：
>（1）考虑一下如何给词频统计后的词做词云  
>（2）修改一下cloud_word_test()，好乱 √2019.1.5  

####2019.1.5  Many def.....
完成内容：
>（1）原来的3个函数硬生生改成了5个。现在只要修改一下字符串textname，并把相应文件放到input_data文件夹下面，运行一下，分词结果就出来了。还算满意～  
>`format_text()`用于格式化文本，返回字符串  
>`cut_word()`用于分词，返回列表  
>`store_word`()用于存储分词结果，返回输出路径  
>`count_word()`用于词频统计，返回输出路径  
>`cloud_word_test()`用于画词云，返回输出路径  
>（2）Wordseg0.0基本完成！   

待完成项：
>（1）返回的输出路径貌似是不专业的，估计可以调用什么系统函数。暂时不会，以后再说。  
>（2）我还是拖拖拖的不想做"教育政策"常用词的词典。。。。。  
>(3)思考一下做完词频干嘛  